{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b886804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0fe7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import datetime\n",
    "import subprocess\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd42ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\vridh\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",\n",
    "                                    use_auth_token=\"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\")\n",
    "\n",
    "\n",
    "path = \"dailylife001.mp3\"\n",
    "if path[-3:] != 'wav':\n",
    "  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
    "  path = 'audio.wav'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6f0dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0.7s stop=3.4s speaker_SPEAKER_01\n",
      "start=4.0s stop=6.1s speaker_SPEAKER_00\n",
      "start=6.7s stop=9.3s speaker_SPEAKER_01\n",
      "start=9.5s stop=11.2s speaker_SPEAKER_00\n",
      "start=11.7s stop=14.8s speaker_SPEAKER_01\n",
      "start=15.0s stop=18.4s speaker_SPEAKER_00\n",
      "start=20.0s stop=22.2s speaker_SPEAKER_01\n",
      "start=22.3s stop=23.3s speaker_SPEAKER_00\n",
      "start=24.5s stop=27.0s speaker_SPEAKER_01\n",
      "start=27.0s stop=30.7s speaker_SPEAKER_00\n",
      "start=31.8s stop=34.8s speaker_SPEAKER_01\n",
      "start=34.8s stop=38.1s speaker_SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# 4. apply pretrained pipeline\n",
    "diarization = pipeline(path)\n",
    "\n",
    "# 5. print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41da97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7346d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Run your speaker diarization pipeline\n",
    "diarization = pipeline(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b248af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dailylife001.mp3\"\n",
    "if path[-3:] != 'wav':\n",
    "  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
    "  path = 'audio.wav'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c316533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: where do you live\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: I live in Pasadena\n",
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: where is Pasadena\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: it's in California\n",
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: is it in Northern California\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: no it's in Southern California\n",
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: is Pasadena a big city\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: it's pretty\n",
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: how big is pretty big\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: it has about a hundred and forty thousand people\n",
      "Converting audio transcripts into text for speaker_SPEAKER_01 ...\n",
      "speaker_SPEAKER_01: how big is Los Angeles\n",
      "Converting audio transcripts into text for speaker_SPEAKER_00 ...\n",
      "speaker_SPEAKER_00: it has about 3 million people\n"
     ]
    }
   ],
   "source": [
    "# For each speaker's turn in the conversation\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    \n",
    "    # Slice the audio file to get only the current speaker's turn\n",
    "    audio_turn = AudioSegment.from_wav('audio.wav')[turn.start * 1000:turn.end * 1000]\n",
    "    audio_turn.export('temp.wav', format='wav')\n",
    "\n",
    "    # Use the SpeechRecognition library to transcribe the speech in the sliced audio file\n",
    "    with sr.AudioFile('temp.wav') as source:\n",
    "        audio_text = r.listen(source)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Converting audio transcripts into text for speaker_{speaker} ...\")\n",
    "            text = r.recognize_google(audio_text)\n",
    "            print(f\"speaker_{speaker}: {text}\")\n",
    "        \n",
    "        except:\n",
    "             print('Sorry.. run again...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
